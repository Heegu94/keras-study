{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization algorithms 정리\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network의 weight을 조절하는 과정에는 보통 ‘Gradient Descent’ 라는 방법을 사용한다. </br> 이는 네트워크의 parameter들을 θ라고 했을 때, 네트워크에서 내놓는 결과값과 실제 결과값 사이의 차이를 정의하는 함수 Loss function J(θ)의 값을 최소화하기 위해 기울기(gradient) ∇θJ(θ)를 이용하는 방법이다. </br> Gradient Descent에서는 θ 에 대해 gradient의 반대 방향으로 일정 크기만큼 이동해내는 것을 반복하여 Loss function J(θ) 의 값을 최소화하는 θ 의 값을 찾는다. </br> 한 iteration에서의 변화 식은 다음과 같다.\n",
    "\n",
    "### θ = θ − η∇θJ(θ)\n",
    "\n",
    "이 때 η 는 미리 정해진 걸음의 크기 ‘step size’ 로서, 보통 0.01~0.001 정도의 적당한 크기를 사용한다.\n",
    "\n",
    "이 때 Loss Function을 계산할 때 전체 train set을 사용하는 것을 **Batch Gradient Descent** 라고 한다. </br>그러나 이렇게 계산을 할 경우 한번 step을 내딛을 때 전체 데이터에 대해 Loss Function을 계산해야 하므로 너무 많은 계산량이 필요하다. </br> 이를 방지하기 위해 보통은 **Stochastic Gradient Descent (SGD)** 라는 방법을 사용한다. </br> 이 방법에서는 loss function을 계산할 때 전체 데이터(batch) 대신 일부 조그마한 데이터의 모음(mini-batch)에 대해서만 loss function을 계산한다. </br> 이 방법은 batch gradient descent 보다 다소 부정확할 수는 있지만, 훨씬 계산 속도가 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있으며 여러 번 반복할 경우 보통 batch의 결과와 유사한 결과로 수렴한다. </br> 또한, SGD를 사용할 경우 Batch Gradient Descent에서 **local minima**에 빠지지 않고 더 좋은 방향으로 수렴할 가능성도 있다.\n",
    "\n",
    "보통 Neural Network를 트레이닝할 때는 이 SGD를 이용한다. </br> 그러나 단순한 SGD를 이용하여 네트워크를 학습시키는 것에는 **한계가 있다**.\n",
    "\n",
    " **SGD가 우리가 알고 있는 Naive Stochastic Gradient Descent 알고리즘**이고, **Momentum, NAG, Adagrad, AdaDelta, RMSprop 등은 SGD의 변형**이다. </br> 보다시피 모든 경우에서 SGD는 다른 알고리즘들에 비해 성능이 월등하게 낮다. </br> 다른 알고리즘들 보다 이동속도가 현저하게 느릴뿐만 아니라, 방향을 제대로 잡지 못하고 이상한 곳에서 수렴하여 이동하지 못하는 모습도 관찰할 수 있다. </br> 즉 단순한 SGD를 이용하여 네트워크를 학습시킬 경우 네트워크가 상대적으로 좋은 결과를 얻지 못할 것이라고 예측할 수 있다. </br> 그렇다면 실제로는 어떤 방법들을 이용해야 하는 것인가? </br> **Neural Network를 학습시킬 때 실제로 많이 사용하는 다양한 SGD의 변형 알고리즘들**을 간략하게 살펴보겠다. </br> 내용과 그림의 상당 부분은 Sebastian Ruder의 글 에서 차용했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Momentum\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Momentum** 방식은 말 그대로 Gradient Descent를 통해 이동하는 과정에 일종의 **‘관성’**을 주는 것이다. </br> 현재 Gradient를 통해 이동하는 방향과는 별개로, 과거에 이동했던 방식을 기억하면서 그 방향으로 일정 정도를 추가적으로 이동하는 방식이다. </br> 수식으로 표현하면 다음과 같다. ' ***vt*** '를 **time step *t* 에서의 이동 벡터** 라고 할 때, 다음과 같은 식으로 이동을 표현할 수 있다.\n",
    "\n",
    "### *vt* = *γvt* − 1 + *η*∇θ*J*(θ)\n",
    "### θ = θ − *vt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때, *γ*는 얼마나 momentum을 줄 것인지에 대한 momentum term으로서, 보통 0.9 정도의 값을 사용한다. </br> 식을 살펴보면 과거에 얼마나 이동했는지에 대한 이동 항 *v*를 기억하고, 새로운 이동항을 구할 경우 과거에 이동했던 정도에 관성항만큼 곱해준 후 Gradient을 이용한 이동 step 항을 더해준다. </br> 이렇게 할 경우 이동항 *vt* 는 다음과 같은 방식으로 정리할 수 있어, **Gradient들의 지수평균을 이용하여 이동한다고도 해석**할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *vt* = *η*∇θJ(θ)*(t)* + *γη*∇θJ(θ)*(t−1)* + *γ2η*∇θJ(θ)*(t−2)* + ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 SGD를 이용할 경우, local minima에 빠지면 gradient가 0이 되어 이동할 수가 없지만, momentum 방식의 경우 기존에 이동했던 방향에 관성이 있어 이 local minima를 빠져나오고 더 좋은 minima로 이동할 것을 기대할 수 있게 된다. </br> 반면 momentum 방식을 이용할 경우 기존의 변수들 θ 외에도 과거에 이동했던 양을 변수별로 저장해야하므로 변수에 대한 메모리가 기존의 두 배로 필요하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Adagrad\n",
    "\n",
    "___\n",
    "\n",
    "**Adagrad(Adaptive Gradient)** 는 변수들을 update할 때 각각의 변수마다 step size를 다르게 설정해서 이동하는 방식이다. </br> 이 알고리즘의 기본적인 아이디어는 **‘지금까지 많이 변화하지 않은 변수들은 step size를 크게 하고, 지금까지 많이 변화했던 변수들은 step size를 작게 하자’** 라는 것이다. </br> 자주 등장하거나 변화를 많이 한 변수들의 경우 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 세밀한 값을 조정하고, </br> 적게 변화한 변수들은 optimum 값에 도달하기 위해서는 많이 이동해야할 확률이 높기 때문에 먼저 빠르게 loss 값을 줄이는 방향으로 이동하려는 방식이라고 생각할 수 있겠다. </br> 특히 **word2vec**이나 **GloVe** 같이 **word representation**을 학습시킬 경우 단어의 등장 확률에 따라 variable의 사용 비율이 확연하게 차이나기 때문에 **Adagrad**와 같은 학습 방식을 이용하면 훨씬 더 좋은 성능을 거둘 수 있을 것이다.\n",
    "\n",
    "**Adagrad**의 한 스텝을 수식화하여 나타내면 다음과 같다.\n",
    "\n",
    "![img_2](./imgs/2.png)\n",
    "\n",
    "Neural Network의 parameter가 **k** 개라고 할 때, **Gt**는 k차원 벡터로서 **‘time step t까지 각 변수가 이동한 gradient의 sum of squares’** 를 저장한다. </br> θ를 업데이트하는 상황에서는 기존 **step size η**에 **Gt의 루트값에 반비례한 크기로 이동**을 진행하여, 지금까지 많이 변화한 변수일수록 적게 이동하고 적게 변화한 변수일수록 많이 이동하도록 한다. 이 때 **ϵ**은 10−4 ~ 10−8 정도의 작은 값으로서 **0으로 나누는 것을 방지하기 위한 작은 값**이다. 여기에서 **Gt**를 업데이트하는 식에서 **제곱은 element-wise 제곱을 의미**하며, **θ를 업데이트하는 식**에서도** ⋅ 은 element-wise한 연산을 의미**한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp은 딥러닝의 대가 제프리 힌톤이 제안한 방법으로서, Adagrad의 단점을 해결하기 위한 방법이다. </br> Adagrad의 식에서 gradient의 제곱값을 더해나가면서 구한 *Gt* 부분을 **합이 아니라 지수평균으로 바꾸어서 대체한 방법**이다. </br> 이렇게 대체를 할 경우 Adagrad처럼 *Gt*가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있다. 식으로 나타내면 다음과 같다.\n",
    "\n",
    "![img_1](./imgs/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Adam (Adaptive Moment Estimation)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam (Adaptive Moment Estimation)** 은 RMSProp과 Momentum 방식을 합친 것 같은 알고리즘이다. </br> 이 방식에서는 Momentum 방식과 유사하게 **지금까지 계산해온 기울기의 지수평균을 저장**하며, RMSProp과 유사하게 **기울기의 제곱값의 지수평균을 저장**한다. </br>\n",
    "\n",
    "![img_3](./imgs/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다만, Adam에서는 *m*과 *v*가 처음에 *0* 으로 초기화되어 있기 때문에 학습의 초반부에서는 *mt,vt*가 0에 가깝게 bias 되어있을 것이라고 판단하여 이를 **unbiased 하게 만들어주는 작업**을 거친다. </br> *mt 와 vt*의 식을 *∑ 형태*로 펼친 후 양변에 expectation을 씌워서 정리해보면, 다음과 같은 보정을 통해 *unbiased 된 expectation*을 얻을 수 있다. </br> 이 보정된 expectation들을 가지고 gradient가 들어갈 자리에 *^mt*, *Gt*가 들어갈 자리에 *^vt* 를 넣어 계산을 진행한다.\n",
    "\n",
    "![img_4](./imgs/4.png)\n",
    "\n",
    "보통 *β1* 로는 0.9, *β2*로는 0.999, *ϵ* 으로는 *10^−8* 정도의 값을 사용한다고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Summary\n",
    "___\n",
    "\n",
    "지금까지 다양한 Gradient Descent 알고리즘의 변형 알고리즘들을 알아보았다. </br> Momentum, NAG, AdaGrad, AdaDelta, RMSProp, Adam 등 다양한 알고리즘들이 있었지만 이 중에서 어느 알고리즘이 가장 좋다, 라고 말하기는 힘들다. </br> 어떤 문제를 풀고있는지, 어떤 데이터셋을 사용하는지, 어떤 네트워크에 대해 적용하는지에 따라 각 방법의 성능은 판이하게 차이가 날 것이므로 실제로 네트워크를 학습시킬 때는 다양한 시도를 해보며 현재 경우에서는 어떤 알고리즘이 가장 성능이 좋은지에 대해 실험해볼 필요가 있다. </br> 다행히, Tensorflow 등의 Machine learning library를 사용하면 코드 한 줄만 변화시키면 쉽게 어떤 optimizer를 사용할 것인지 설정해줄 수 있어 간편하게 실험해볼 수 있을 것이다.\n",
    "\n",
    "여기서 설명한 알고리즘들은 모두 Stochastic Gradient Descent, 즉 단순한 first-order optimization의 변형들이다. </br> 이 외에도 Newton’s Method 등 second-order optimization을 기반으로 한 알고리즘들도 있다. </br> 그러나 단순한 second-order optimization을 사용하기 위해서는 Hessian Matrix라는 2차 편미분 행렬을 계산한 후 역행렬을 구해야 하는데, 이 계산과정이 계산적으로 비싼 작업이기 때문에 보통 잘 사용되지 않는다. </br> 이러한 계산량을 줄이기 위해 hessian matrix를 근사하거나 추정해나가면서 계산을 진행하는 BFGS / L-BFGS 등의 알고리즘, 그리고 hessian matrix를 직접 계산하지 않으면서 second-order optimization인 Hessian-Free Optimization 등도 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## REFERENCE\n",
    "\n",
    "1. BEOMSU KIM's BLOG 참조 및 인용 (거의 대부분).\n",
    "\n",
    "http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html\n",
    "\n",
    "\n",
    "2. Sebastian Ruder의 글 참조.\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
